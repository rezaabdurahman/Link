global:
  smtp_smarthost: '${SMTP_HOST}:587'
  smtp_from: 'alerts@yourdomain.com'
  smtp_auth_username: '${SMTP_USER}'
  smtp_auth_password: '${SMTP_PASSWORD}'
  slack_api_url: '${SLACK_WEBHOOK_URL}'
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  receiver: 'production-default'
  routes:
    # Critical alerts go to PagerDuty and immediate notifications
    - match:
        severity: critical
      receiver: 'production-critical'
      repeat_interval: 30s
      continue: true
    
    # Service down alerts trigger immediate response
    - match:
        alertname: ServiceDown
      receiver: 'production-emergency'
      repeat_interval: 30s
      continue: true
    
    # Database issues trigger immediate response
    - match_re:
        alertname: '(PostgreSQLDown|DatabaseConnectionPoolHigh|HighQueryDuration)'
      receiver: 'production-database'
      repeat_interval: 1m
      continue: true
    
    # Security-related alerts
    - match_re:
        alertname: '(HighJWTValidationFailures|SecurityBreach|UnauthorizedAccess)'
      receiver: 'production-security'
      repeat_interval: 30s
      continue: true
    
    # Performance warnings
    - match:
        severity: warning
      receiver: 'production-warning'
      repeat_interval: 2h

receivers:
  - name: 'production-default'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#link-production-alerts'
        title: '[PRODUCTION] Link App Alert'
        text: |
          *Environment*: Production üî¥
          *Alert*: {{ .GroupLabels.alertname }}
          *Summary*: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
          *Description*: {{ range .Alerts }}{{ .Annotations.description }}{{ end }}
          *Time*: {{ range .Alerts }}{{ .StartsAt }}{{ end }}
        send_resolved: true

  - name: 'production-critical'
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_ROUTING_KEY}'
        description: '[CRITICAL] {{ .GroupLabels.alertname }}'
        details:
          alert: '{{ .GroupLabels.alertname }}'
          summary: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
          environment: 'production'
        links:
          - href: 'https://grafana.yourdomain.com'
            text: 'Grafana Dashboard'
          - href: 'https://prometheus.yourdomain.com'
            text: 'Prometheus'

    email_configs:
      - to: 'oncall@yourdomain.com,cto@yourdomain.com'
        subject: '[üö® CRITICAL PRODUCTION] {{ .GroupLabels.alertname }}'
        headers:
          Priority: 'urgent'
        body: |
          CRITICAL PRODUCTION ALERT
          
          Environment: Production
          Alert: {{ .GroupLabels.alertname }}
          Time: {{ range .Alerts }}{{ .StartsAt }}{{ end }}
          
          {{ range .Alerts }}
          Summary: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Instance: {{ .Labels.instance }}
          Service: {{ .Labels.job }}
          {{ end }}
          
          Runbook: {{ range .Alerts }}{{ .Annotations.runbook_url }}{{ end }}
          
          Dashboard Links:
          - Grafana: https://grafana.yourdomain.com
          - Prometheus: https://prometheus.yourdomain.com
          - Jaeger: https://jaeger.yourdomain.com

    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#link-critical-production'
        title: '[üö® CRITICAL PRODUCTION] {{ .GroupLabels.alertname }}'
        text: |
          *Environment*: Production üî¥
          *Alert*: {{ .GroupLabels.alertname }}
          *Summary*: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
          *Description*: {{ range .Alerts }}{{ .Annotations.description }}{{ end }}
          {{ range .Alerts }}*Instance*: {{ .Labels.instance }}{{ end }}
          {{ range .Alerts }}*Service*: {{ .Labels.job }}{{ end }}
          *Time*: {{ range .Alerts }}{{ .StartsAt }}{{ end }}
          
          <https://grafana.yourdomain.com|üìä Grafana> | <https://prometheus.yourdomain.com|üéØ Prometheus> | <https://jaeger.yourdomain.com|üîç Jaeger>
        actions:
          - type: button
            text: 'Acknowledge'
            url: 'https://alertmanager.yourdomain.com'
          - type: button
            text: 'Runbook'
            url: '{{ range .Alerts }}{{ .Annotations.runbook_url }}{{ end }}'
        send_resolved: true

  - name: 'production-emergency'
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_ROUTING_KEY}'
        severity: 'critical'
        description: '[EMERGENCY] Service Down in Production'
        details:
          alert: '{{ .GroupLabels.alertname }}'
          service: '{{ .GroupLabels.job }}'
          environment: 'production'

    # Immediate phone calls for service down
    webhook_configs:
      - url: '${PHONE_ALERT_WEBHOOK}'
        send_resolved: false

    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#link-emergency'
        title: '[üî¥ EMERGENCY] Service Down in Production'
        text: |
          @channel PRODUCTION SERVICE DOWN
          
          *Service*: {{ .GroupLabels.job }}
          {{ range .Alerts }}*Instance*: {{ .Labels.instance }}{{ end }}
          *Time*: {{ range .Alerts }}{{ .StartsAt }}{{ end }}
          
          Immediate action required!
        send_resolved: true

  - name: 'production-database'
    email_configs:
      - to: 'dba@yourdomain.com,oncall@yourdomain.com'
        subject: '[DATABASE ALERT - PRODUCTION] {{ .GroupLabels.alertname }}'
        body: |
          DATABASE ALERT - PRODUCTION
          
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Database: {{ .Labels.database }}
          Instance: {{ .Labels.instance }}
          {{ end }}

    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#link-database-alerts'
        title: '[üíæ DATABASE - PRODUCTION] {{ .GroupLabels.alertname }}'
        text: |
          *Environment*: Production üî¥
          {{ range .Alerts }}*Database*: {{ .Labels.database }}{{ end }}
          {{ range .Alerts }}*Instance*: {{ .Labels.instance }}{{ end }}
          *Alert*: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}

  - name: 'production-security'
    email_configs:
      - to: 'security@yourdomain.com,ciso@yourdomain.com'
        subject: '[üîí SECURITY ALERT - PRODUCTION] {{ .GroupLabels.alertname }}'
        headers:
          Priority: 'urgent'
        body: |
          SECURITY ALERT - PRODUCTION ENVIRONMENT
          
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Source: {{ .Labels.instance }}
          Time: {{ .StartsAt }}
          {{ end }}
          
          Immediate investigation required.

    slack_configs:
      - api_url: '${SECURITY_SLACK_WEBHOOK}'
        channel: '#security-alerts'
        title: '[üîí SECURITY - PRODUCTION] {{ .GroupLabels.alertname }}'
        text: |
          @here SECURITY ALERT - Production
          
          *Alert*: {{ .GroupLabels.alertname }}
          {{ range .Alerts }}*Source*: {{ .Labels.instance }}{{ end }}
          *Summary*: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
          
          Immediate investigation required.
        send_resolved: true

  - name: 'production-warning'
    email_configs:
      - to: 'dev-team@yourdomain.com'
        subject: '[WARNING - PRODUCTION] {{ .GroupLabels.alertname }}'
        body: |
          Production Warning Alert
          
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Service: {{ .Labels.job }}
          Instance: {{ .Labels.instance }}
          {{ end }}

    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#link-production-alerts'
        title: '[‚ö†Ô∏è WARNING - PRODUCTION] {{ .GroupLabels.alertname }}'
        text: |
          *Environment*: Production üî¥
          *Alert*: {{ .GroupLabels.alertname }}
          {{ range .Alerts }}*Service*: {{ .Labels.job }}{{ end }}
          *Summary*: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
        send_resolved: true

inhibit_rules:
  # Don't alert on warnings if there's a critical alert for the same service
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'cluster', 'service']
  
  # Don't alert on high error rate if service is already down
  - source_match:
      alertname: 'ServiceDown'
    target_match:
      alertname: 'HighErrorRate'
    equal: ['job', 'instance']